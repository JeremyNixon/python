{
 "metadata": {
  "name": "",
  "signature": "sha256:9d6da27a443fbf779d8b02d8427441378e9f22e8e60361e127b4807d5b0cba9a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from mailbox import mbox\n",
      "import pandas as pd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"hello world\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "hello world\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def store_content(message, body=None):\n",
      "    if not body:\n",
      "        body = message.get_payload(decode=True)\n",
      "    if len(message):\n",
      "        contents = {\n",
      "            \"subject\": message['subject'] or \"\",\n",
      "            \"body\": body,\n",
      "            \"from\": message['from'],\n",
      "            \"to\": message['to'],\n",
      "            \"date\": message['date'],\n",
      "            \"labels\": message['X-Gmail-Labels'],\n",
      "            \"epilogue\": message.epilogue,\n",
      "        }\n",
      "        return df.append(contents, ignore_index=True)\n",
      "\n",
      "# Create an empty DataFrame with the relevant columns\n",
      "df = pd.DataFrame(\n",
      "    columns=(\"subject\", \"body\", \"from\", \"to\", \"date\", \"labels\", \"epilogue\"))\n",
      "\n",
      "# Import your downloaded mbox file\n",
      "box = mbox('/Users/Jeremy/Downloads/All mail Including Spam and Trash-2.mbox')\n",
      "\n",
      "fails = []\n",
      "for message in box:\n",
      "    try:\n",
      "        if message.get_content_type() == 'text/plain':\n",
      "            df = store_content(message)\n",
      "        elif message.is_multipart():\n",
      "            # Grab any plaintext from multipart messages\n",
      "            for part in message.get_payload():\n",
      "                if part.get_content_type() == 'text/plain':\n",
      "                    df = store_content(message, part.get_payload(decode=True))\n",
      "                    break\n",
      "    except:\n",
      "        fails.append(message)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import Counter\n",
      "\n",
      "subject_word_bag = df.subject.apply(lambda t: t.lower() + \" \").sum()\n",
      "\n",
      "Counter(subject_word_bag.split()).most_common()[:10]\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "[('re:', 10848),\n",
        " ('on', 5729),\n",
        " ('redline]', 4796),\n",
        " ('[a-team', 4796),\n",
        " ('you', 3323),\n",
        " ('a', 2937),\n",
        " ('to', 2400),\n",
        " ('the', 2171),\n",
        " ('commented', 2141),\n",
        " ('your', 2050)]"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import brown\n",
      "brown.words()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import stopwords\n",
      "stops = [unicode(word) for word in stopwords.words('english')] + ['re:', 'fwd:', '-']\n",
      "subject_words = [word for word in subject_word_bag.split() if word.lower() not in stops]\n",
      "Counter(subject_words).most_common()[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "-c:3: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "[('redline]', 4796),\n",
        " ('[a-team', 4796),\n",
        " ('commented', 2141),\n",
        " ('facebook...', 1522),\n",
        " ('card', 1429),\n",
        " ('[wild', 1315),\n",
        " ('2013]', 1309),\n",
        " ('[debate-novices]', 1183),\n",
        " ('status...', 1155),\n",
        " ('practice', 1063)]"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk import collocations\n",
      "bigram_measures = collocations.BigramAssocMeasures()\n",
      "bigram_finder = collocations.BigramCollocationFinder.from_words(subject_words)\n",
      "\n",
      "# Filter to top 20 results; otherwise this will take a LONG time to analyze\n",
      "bigram_finder.apply_freq_filter(20)\n",
      "for bigram in bigram_finder.score_ngrams(bigram_measures.raw_freq)[:10]:\n",
      "    print bigram\n",
      " "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(('[a-team', 'redline]'), 0.037010456457151675)\n",
        "(('[wild', 'card'), 0.010147779449781997)\n",
        "(('card', '2013]'), 0.01010147779449782)\n",
        "(('[bred', 'line]'), 0.0061581201527954625)\n",
        "(('commented', 'status...'), 0.005533047806459081)\n",
        "(('redline]', 'practice'), 0.0045144113902072)\n",
        "(('friend', 'facebook...'), 0.004159432033028514)\n",
        "(('commented', 'photo'), 0.004059111779912798)\n",
        "(('photo', 'facebook...'), 0.004028244009723347)\n",
        "(('also', 'commented'), 0.002562024925724428)\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for bigram in bigram_finder.nbest(bigram_measures.pmi, 5):\n",
      "    print bigram\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('-source', 'http://hcs.harvard.edu/~disc/cron.php')\n",
        "('/usr/bin/lynx', '-source')\n",
        "('<disc@caesar>', '/usr/bin/lynx')\n",
        "('cron', '<disc@caesar>')\n",
        "('story!', 'felix,')\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from textblob import TextBlob\n",
      "df['feels'] = df.subject.apply(\n",
      "    lambda s: TextBlob(unicode(s, errors='ignore')).sentiment.polarity)\n",
      "\n",
      "# Output a few subject lines with their calculated sentiment scores\n",
      "df = df.sort(columns = ['feels'])\n",
      "print df[['subject', 'feels']]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "                                                 subject  feels\n",
        "14908           Re: [Debate-line] National Champions!!!!     -1\n",
        "18084                  Re: [Debate-novices] THANK YOU!!!     -1\n",
        "16046                   Re: [A-Team Redline] PRACTICE!!!     -1\n",
        "16045                   Re: [A-Team Redline] PRACTICE!!!     -1\n",
        "16044                   Re: [A-Team Redline] PRACTICE!!!     -1\n",
        "19839                       Really Terrible Study Breaks     -1\n",
        "17718                       Re: [A-Team Redline] HALP!!!     -1\n",
        "14218              Re: [A-Team Redline] LEZZZZ GOOOOOO!!     -1\n",
        "17717                       Re: [A-Team Redline] HALP!!!     -1\n",
        "17716                       Re: [A-Team Redline] HALP!!!     -1\n",
        "27775              Coupon, Cupid & the Bard's Worst Fear     -1\n",
        "19819       Re: [Debate-novices] [Debate-line] 50%! 50%!     -1\n",
        "16111             Re: [Debate-novices] Judge at 7:20!!!!     -1\n",
        "19393  [Education] Fwd: [Policygeneral] Policy Evalua...     -1\n",
        "9333   Re: [A-Team Redline] Ping pong tourney TODAYYY...     -1\n",
        "16043                   Re: [A-Team Redline] PRACTICE!!!     -1\n",
        "16042                   Re: [A-Team Redline] PRACTICE!!!     -1\n",
        "16041                   Re: [A-Team Redline] PRACTICE!!!     -1\n",
        "8911                             Fwd: A terrible tragedy     -1\n",
        "16502  Re: [A-Team Redline] Re: Practice Tonight TIME...     -1\n",
        "16501  Re: [A-Team Redline] Re: Practice Tonight TIME...     -1\n",
        "19820       Re: [Debate-novices] [Debate-line] 50%! 50%!     -1\n",
        "7189                    Re: [A-Team Redline] Practice!!!     -1\n",
        "16110             Re: [Debate-novices] Judge at 7:20!!!!     -1\n",
        "19825       Re: [Debate-novices] [Debate-line] 50%! 50%!     -1\n",
        "17713                       Re: [A-Team Redline] HALP!!!     -1\n",
        "17714                       Re: [A-Team Redline] HALP!!!     -1\n",
        "17715                       Re: [A-Team Redline] HALP!!!     -1\n",
        "14906  Re: [Debate-novices] [Debate-line] National Ch...     -1\n",
        "24589  John Schlenner suggested you like I hate it wh...     -1\n",
        "...                                                  ...    ...\n",
        "24324  2 great presentations today in geology - Aleut...      1\n",
        "22826  The best scores are in from our Practice Test ...      1\n",
        "11909                 [Debate-line] HAPPY BIRTHDAY JAKE!      1\n",
        "24333                    COUPON + Great Gifts for Grads!      1\n",
        "11211                  Welcome Madeline Zhu to LinkedIn!      1\n",
        "9141     Jeremy, there's still time to Lose It! and win!      1\n",
        "25234  Tahra Gribbin invited you to the event \"My Bir...      1\n",
        "11881                  Welcome William Ryan to LinkedIn!      1\n",
        "20116            [Policygeneral] BE A MAL! It's awesome.      1\n",
        "5274                                 Welcome to GroupMe!      1\n",
        "28083                     Welcome to the Forensics Team!      1\n",
        "20672                        Welcome to USAUltimate.org!      1\n",
        "12393                                 Welcome to Pocket!      1\n",
        "21114       [Debate-novices] Awesome things to do today!      1\n",
        "19223  Dad Deserves the Best! Save 15% on a Gift to M...      1\n",
        "5047                  [A-Team Redline] Awesome MLU Stats      1\n",
        "8721   [Debate-novices] A break from the tournament f...      1\n",
        "28639               30% Coupon -- Our Best Deal for You       1\n",
        "4730   Thank you for subscribing! You are completely ...      1\n",
        "26572     Welcome to Express! Save 15% through August 18      1\n",
        "19469           Four Great Reasons to Shop BN.com Today!      1\n",
        "9251                   Welcome Alex Edinger to LinkedIn!      1\n",
        "10372                      The best flight deals for you      1\n",
        "28650  boy, I can't wait to see the replies for this ...      1\n",
        "28651  Re: boy, I can't wait to see the replies for t...      1\n",
        "28652  Re: boy, I can't wait to see the replies for t...      1\n",
        "17335  [bostonfrisbee] Happy Thanksgiving and Merry T...      1\n",
        "20326  FUNdraising Day RIGHT NOW, Ticknor Lounge!!!! ...      1\n",
        "19902  [Policygeneral] COME TO THE BEST PARTY OF THE ...      1\n",
        "16708  IFBB Pro Shawn Ray EXCLUSIVE workout routine, ...      1\n",
        "\n",
        "[28956 rows x 2 columns]\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}